{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.bert.tokenization_bert import BertTokenizer\n",
    "decoder_model_path = 'xxx'\n",
    "tokenizer = BertTokenizer.from_pretrained(decoder_model_path)\n",
    "special_tokens_dict = {'bos_token': '<BOS>', 'eos_token': '<EOS>'}\n",
    "tokenizer.add_special_tokens(special_tokens_dict)\n",
    "tokenizer.unk_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def get_decoder_beta_list( n_steps, start=0., stop=1.0, n_cycle=4):\n",
    "        L = np.ones(n_steps)\n",
    "        t_range = int(n_steps / n_cycle)\n",
    "        for t_cur in range(n_steps):\n",
    "            if t_cur > t_range:\n",
    "                L[t_cur] = 0.\n",
    "            else:    \n",
    "                ratio = t_cur / t_range\n",
    "                value = stop - ratio * (stop-start)\n",
    "                L[t_cur] = value\n",
    "        return L\n",
    "get_decoder_beta_list(100, 0, 1, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import sys\n",
    "\n",
    "import json\n",
    "import torch\n",
    "import argparse\n",
    "from vae_pl_module import TDVAEModule\n",
    "from load import ChineseSentenceSplitter\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '5'\n",
    "\n",
    "def TDVAECollator(samples):\n",
    "    # when len(samples) is larger than one, we need to save the sentence length info \n",
    "    paragraph_lengths = []\n",
    "    encoder_inputs, decoder_labels = [], []\n",
    "    for sp in samples:\n",
    "        # NOTE: in TD-VAE, both encoder and decoder are gpt2, thus use decoder sent twice !\n",
    "        sent_lengths = sp['decoder_sent_lengths']\n",
    "        input_ids, decoder_target = sp['decoder_target'], sp['decoder_target']\n",
    "        if len(sent_lengths) == 1 or sent_lengths[0] >= 512 or sent_lengths[0] + sent_lengths[1] >= 512:\n",
    "            continue # we ignore paragraphs with only one sentence split\n",
    "        encoder_inputs.append(torch.tensor(input_ids[:512], dtype=torch.long))\n",
    "        decoder_labels.append(torch.tensor(decoder_target[:512], dtype=torch.long))\n",
    "        paragraph_lengths.append(sent_lengths)\n",
    "    if not encoder_inputs or not decoder_labels:\n",
    "        return None, None, None  # if all the examples in the batch are single sentence\n",
    "    encoder_inputs = pad_sequence(encoder_inputs, batch_first=True, padding_value=0)\n",
    "    decoder_labels = pad_sequence(decoder_labels, batch_first=True, padding_value=0)\n",
    "    return (encoder_inputs, decoder_labels, paragraph_lengths) \n",
    "\n",
    "\n",
    "# parse arguments\n",
    "mlm_probability = 0.\n",
    "\n",
    "checkpoint_path = 'xxx'\n",
    "# marf_checkpoint_path = 'xxx'\n",
    "encoder_model_path = 'xxx'\n",
    "decoder_model_path = 'xxx'\n",
    "max_split_num = 12\n",
    "\n",
    "args_parser = argparse.ArgumentParser()\n",
    "args_parser.add_argument(\"--checkpoint_path\", type=str, default=checkpoint_path)\n",
    "# args_parser.add_argument(\"--marf_checkpoint_path\", type=str, default=marf_checkpoint_path)\n",
    "args_parser.add_argument(\"--encoder_model_path\", type=str, default=encoder_model_path)\n",
    "args_parser.add_argument(\"--decoder_model_path\", type=str, default=decoder_model_path)\n",
    "args_parser.add_argument(\"--mlm_probability\", type=float, default=mlm_probability)\n",
    "args_parser.add_argument(\"--max_split_num\", type=int, default=max_split_num)\n",
    "args, unknown_args = args_parser.parse_known_args()\n",
    "\n",
    "# load model\n",
    "model, encoder_tokenizer, decoder_tokenizer, latent_size, \\\n",
    "    labels_dict, args =  TDVAEModule.load_model(args, labels_dict=None)\n",
    "\n",
    "# load and process data \n",
    "sentence_splitter = ChineseSentenceSplitter()\n",
    "data = []\n",
    "inputs_dicts = []\n",
    "bos_token, eos_token = decoder_tokenizer.bos_token_id, decoder_tokenizer.eos_token_id\n",
    "with open(\"xxx/wudao/dev.json\") as file:\n",
    "    lines = file.readlines()\n",
    "    for line in lines:\n",
    "        data_dict = json.loads(line)    \n",
    "        data.append(sentence_splitter.tokenize(data_dict['content']))\n",
    "    for sentences in data:\n",
    "        decoder_sent_lengths, decoder_target = [], []\n",
    "        for sentence in sentences:\n",
    "            # tokenize sentence with two tokenizer\n",
    "            decoder_sent_target = decoder_tokenizer.convert_tokens_to_ids(decoder_tokenizer.tokenize(sentence))\n",
    "            decoder_sent_lengths.append(len(decoder_sent_target))\n",
    "            decoder_target.extend(decoder_sent_target)\n",
    "        inputs_dicts.append({\n",
    "            'decoder_target': decoder_target,\n",
    "            \"decoder_sent_lengths\": decoder_sent_lengths\n",
    "        })\n",
    "\n",
    "tdvae_inputs = []\n",
    "for input_dict in inputs_dicts:\n",
    "    inputs_tuple = TDVAECollator([input_dict]) # batch_size = 1\n",
    "    if not any([obj is None for obj in inputs_tuple]):\n",
    "        tdvae_inputs.append(inputs_tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 50\n",
    "top_p = 0.3\n",
    "temperature = 1.\n",
    "repetition_penalty = 1.\n",
    "device = 0\n",
    "model = model.eval()\n",
    "model.transition_net.model = model.transition_net.model.eval()\n",
    "model = model.to(device)\n",
    "inputs_idx = 4\n",
    "inputs = tdvae_inputs[inputs_idx]\n",
    "encoder_inputs, decoder_labels, paragraph_lengths = inputs\n",
    "batch_belief_latent_z = model.get_belief_latent_z(encoder_inputs.to(device), paragraph_lengths, sample=False)\n",
    "f_outputs, logdets = model.transition_net.model(batch_belief_latent_z.to(device), mode='direct')\n",
    "b_outputs, logdets = model.transition_net.model(f_outputs, mode='inverse')\n",
    "outputs = model.inference(encoder_inputs.to(device), paragraph_lengths, decoder_tokenizer, \n",
    "    max_length, top_p, temperature, repetition_penalty, mode=\"belief\", sample_z=False, batch_belief_latent_z=b_outputs)\n",
    "\n",
    "for gen_sentences, groung_truth_para, sent_lens in zip(outputs, encoder_inputs, paragraph_lengths):\n",
    "    accm_len = sent_lens[0]\n",
    "    print(decoder_tokenizer.decode(groung_truth_para[:sent_lens[0]]))\n",
    "    print(\"-\"*20)\n",
    "    for gen_sent, sent_len in zip(gen_sentences, sent_lens[1:]):\n",
    "        print(decoder_tokenizer.decode(groung_truth_para[accm_len:accm_len+sent_len]))\n",
    "        accm_len=accm_len+sent_len\n",
    "        print(decoder_tokenizer.decode(gen_sent))\n",
    "        print(\"-\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 50\n",
    "top_p = 0.3\n",
    "temperature = 1.\n",
    "repetition_penalty = 1.\n",
    "device = 0\n",
    "model = model.eval()\n",
    "model = model.to(device)\n",
    "# predict and decode\n",
    "inputs_idx = 4\n",
    "inputs = tdvae_inputs[inputs_idx]\n",
    "encoder_inputs, decoder_labels, paragraph_lengths = inputs\n",
    "# add a replace to test if decoder knows how to predict different sentence based on the period \n",
    "# encoder_inputs[encoder_inputs == 511] = 100  # replace 。 with UNK \n",
    "        \n",
    "outputs = model.inference(encoder_inputs.to(device), paragraph_lengths, decoder_tokenizer, \n",
    "    max_length, top_p, temperature, repetition_penalty, mode=\"predict\", sample_z=False)\n",
    "\n",
    "for gen_sentences, groung_truth_para, sent_lens in zip(outputs, encoder_inputs, paragraph_lengths):\n",
    "    accm_len = sent_lens[0]\n",
    "    print(decoder_tokenizer.decode(groung_truth_para[:sent_lens[0]]))\n",
    "    print(\"-\"*20)\n",
    "    for gen_sent, sent_len in zip(gen_sentences, sent_lens[1:]):\n",
    "        print(decoder_tokenizer.decode(groung_truth_para[accm_len:accm_len+sent_len]))\n",
    "        accm_len=accm_len+sent_len\n",
    "        print(decoder_tokenizer.decode(gen_sent))\n",
    "        print(\"-\"*20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# eval 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder_inputs.shape, decoder_labels.shape, paragraph_lengths\n",
    "max_length = 50\n",
    "top_p = 0.5\n",
    "temperature = 1.\n",
    "repetition_penalty = 1\n",
    "device = 0\n",
    "model = model.eval()\n",
    "model = model.to(device)\n",
    "finished = False\n",
    "decoder_sent_lengths, decoder_target = [], []\n",
    "context = '我是一名土生土长的中国人。'\n",
    "for i in range(5):\n",
    "    tokenized_text1 = decoder_tokenizer.convert_tokens_to_ids(decoder_tokenizer.tokenize(context))\n",
    "    decoder_sent_target = [bos_token] + tokenized_text1 + [eos_token]\n",
    "    decoder_sent_lengths.append(len(decoder_sent_target))\n",
    "    decoder_target.extend(decoder_sent_target)\n",
    "    encoder_inputs = pad_sequence([torch.tensor(decoder_target, dtype=torch.long)], batch_first=True, padding_value=0)\n",
    "    decoder_labels = pad_sequence([torch.tensor(decoder_target, dtype=torch.long)], batch_first=True, padding_value=0)\n",
    "    paragraph_lengths = [decoder_sent_lengths]\n",
    "    outputs = model.inference(encoder_inputs.to(device), paragraph_lengths, decoder_tokenizer, \n",
    "        max_length, top_p, temperature, repetition_penalty, mode=\"inference\")\n",
    "    context = decoder_tokenizer.decode(outputs[0][0][1:-1]).replace(' ', '')\n",
    "    print(context)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Num Active Units "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sent_mus_vars(model, tdvae_inputs, device, num_para=20):\n",
    "    belief_mus, belief_logvars = None, None\n",
    "    for inputs_idx in range(num_para):\n",
    "        inputs = tdvae_inputs[inputs_idx]\n",
    "        encoder_inputs, decoder_labels, paragraph_lengths = inputs\n",
    "        # each list in batch_belief_states contain a tensor with shape Num_sents * H. Num_sents vary across batch\n",
    "        batch_belief_states, batch_actual_para_lengths = model.encoder(encoder_inputs.to(device), paragraph_lengths, \n",
    "            max_split_num=model.max_split_num, unk_token_id=model.unk_token_id)\n",
    "        for belief_states in batch_belief_states:\n",
    "            # Num_sents, Hidden_size => Num_sents, Latent_size\n",
    "            belief_sent_mus, belief_sent_logvars, belief_latent_z = model.get_belief_network_output(belief_states, sample=False)\n",
    "            \n",
    "            belief_mus = belief_sent_mus.detach().clone().cpu() if belief_mus is None else torch.cat((belief_mus, belief_sent_mus.detach().clone().cpu()), dim=0)\n",
    "            belief_logvars = belief_sent_logvars.detach().clone().cpu() if belief_logvars is None else torch.cat((belief_logvars, belief_sent_logvars.detach().clone().cpu()), dim=0)\n",
    "            \n",
    "\n",
    "    return belief_mus, belief_logvars\n",
    "\n",
    "\n",
    "def cal_active_units(z_vector, threshold=0.1):\n",
    "    return torch.sum(torch.var(z_vector, dim=0) > threshold)\n",
    "\n",
    "device = 0\n",
    "model = model.eval()\n",
    "model = model.to(device)\n",
    "belief_mus, belief_logvars = get_sent_mus_vars(model,\n",
    "    tdvae_inputs, device, num_para=50)\n",
    "cal_active_units(belief_mus, 0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "belief_mus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(5):\n",
    "    print(belief_mus[idx].mean(), belief_mus[idx].median(), belief_mus[idx].max(), belief_mus[idx].min())\n",
    "    print(belief_logvars[idx].mean(), belief_logvars[idx].median(), belief_logvars[idx].max(), belief_logvars[idx].min())\n",
    "    print(\"-\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "belief_mus[:10, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_cyclic_linear_beta_list( n_steps, start=0.5, stop=1.0, n_cycle=1):\n",
    "    L = np.ones(n_steps)\n",
    "    t_range = int(n_steps / n_cycle)\n",
    "    for t_cur in range(n_steps):\n",
    "        loc = t_cur % t_range\n",
    "        split_range = int(t_range * (1/3))\n",
    "        if loc < split_range:\n",
    "            ratio = (loc % split_range) / split_range\n",
    "            value = start + ratio * (stop-start)\n",
    "        elif split_range <= loc < 2 * split_range:\n",
    "            value = stop\n",
    "        else:\n",
    "            value = 0\n",
    "        L[t_cur] = value\n",
    "    return L \n",
    "x = get_cyclic_linear_beta_list(1000, 0.5, 1, 10)\n",
    "plt.plot(x)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np \n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "# a = torch.randn(size=(3,64)).to(0)\n",
    "# b = torch.randn(size=(5,64)).to(0)\n",
    "# c = torch.randn(size=(4,32)).to(0)\n",
    "# d = torch.cat((a[0],b[0],c[2]),dim=0)\n",
    "# e = torch.cat((a[1],b[1],c[1]),dim=0)\n",
    "# f torch.sum(torch.stack((d,e), dim=0),dim=1)/torch.tensor([12,24],device=0)\n",
    "def word_drop(x, p, pad_token, vocab_size):     # drop words with probability p\n",
    "    x_ = []\n",
    "    words = x.tolist()\n",
    "    keep = np.random.rand(len(words)) > p\n",
    "    keep[0] = True  # do not drop the start sentence symbol\n",
    "    for j, w in enumerate(words):\n",
    "        if keep[j]:\n",
    "            x_.append(w)\n",
    "        else:\n",
    "            if np.random.rand() > .5:\n",
    "                x_.append(pad_token)\n",
    "            else:\n",
    "                x_.append(np.random.randint(0, vocab_size))\n",
    "    return torch.LongTensor(x_).contiguous().to(x.device)\n",
    "x = torch.randint(0, 1000, size=(1,30))\n",
    "word_drop(x[0], 0.2, 10001, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "tok = BertTokenizer.from_pretrained(\"/cognitive_comp/wanghao/models/gpt2-base\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
